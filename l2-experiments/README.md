# ℓ₂-Experiments

This repository contains code for investigating the _ℓ₂-distance rebound phenomenon_ in neural network training, where student networks initially approach but then progressively diverge from teacher network parameters during training, even while improving functionally.

## Overview

The experiments explore parameter recovery in teacher-student setups using Fefferman-Markel network architectures with `tanh` activation functions. Despite theoretical guarantees suggesting networks should only differ by known symmetry operations, we observe systematic parameter divergence that challenges assumptions about functional-parameter coupling in neural networks.

## Key Findings

Our experiments reveal three distinct phenomena:

1. **Perturbation Recovery**: Student networks initialized with small perturbations of teacher parameters initially approach the teacher but then diverge significantly during training
2. **Noisy Dataset Learning**: Students initialized exactly as teachers but trained on noisy data show monotonic parameter divergence
3. **Noise Sensitivity**: Nearly linear relationship (log-log scale) between dataset noise and final parameter distance

## Experimental Setup

### Core Experiments

The current implementation (`l2_vs_epoch.jl`) supports two complementary experiments:

#### Experiment A: Perturbation Recovery
- **Goal**: Test recovery from small parameter perturbations
- **Setup**: `initial_permut > 0`, `noise_σ = 0`
- **Student initialization**: Teacher parameters + small random perturbation
- **Training data**: Clean, noise-free dataset generated by teacher

#### Experiment B: Noisy Dataset Learning  
- **Goal**: Study parameter drift under data uncertainty
- **Setup**: `initial_permut = 0`, `noise_σ > 0`
- **Student initialization**: Exact copy of teacher parameters
- **Training data**: Teacher-generated dataset with Gaussian noise

*Note: A separate script implementing Experiment C (combined perturbation and noise) will be added in the future.*

## Parameter Configuration

### Core Training Parameters

```julia
learning_rate = 1e-2        # Gradient descent step size
noise_σ = 1e-3              # Standard deviation of Gaussian noise added to training data
initial_permut = 0.0        # Perturbation strength for student initialization
seed = 42                   # Random seed for reproducibility
epochs = 500                # Number of training iterations
num_runs = 10               # Independent runs for statistical averaging
dataset_size = 10000        # Training dataset size
test_set_size = 1000        # Test dataset size
projection_frequency = 1    # Projection onto manifold ℱ frequency
teacher_dimensions = [2, 25, 25, 1]  # Network architecture [input, hidden, hidden, output]
```

### Parameter Details

**Student Initialization Perturbation**: Applied parameter-wise as:
```
θᵢ ← θᵢ + initial_permut × xᵢ
```
where `xᵢ ~ U([-1,1])` and `initial_permut` controls perturbation strength.

**Dataset Noise**: Gaussian noise added to teacher-generated labels:
```
y_noisy = y_teacher + ε,  ε ~ N(0, noise_σ²)
```

**Projection Frequency**: Controls constraint enforcement on special MLP class ℱ:
- `projection_frequency = 1`: Full operation on manifold ℱ (project every epoch)
- `projection_frequency = 0`: No projection (standard unconstrained training)
- `projection_frequency = n > 1`: Project every n epochs

## Usage

### Basic Execution

0. **Build Julia environment** (first time only):
```bash
cd l2-experiments
julia --project=. -e "using Pkg; Pkg.instantiate()"
```

1. **Activate Julia environment**:
```julia
using Pkg
Pkg.activate(".")
```

2. **Configure experiment parameters** in `l2_vs_epoch.jl` (see parameter section above)

3. **Run experiment**:
```julia
julia l2_vs_epoch.jl
```

### Experimental Configurations

**For Perturbation Recovery (Experiment A)**:
```julia
initial_permut = 1e-6  # Small perturbation
noise_σ = 0.0          # Clean data
```

**For Noisy Dataset Learning (Experiment B)**:
```julia
initial_permut = 0.0   # Perfect initialization
noise_σ = 1e-3         # Noisy data
```

**For Noise Sensitivity Analysis**:
_follows in future version_

## Output and Results

### Generated Plots

The experiment produces a two-panel visualization:

**Upper Panel**: ℓ₂-distance evolution between student and teacher parameters
- Shows the characteristic "rebound" phenomenon
- Orange line with normalized values [0,1]

**Lower Panel**: Loss evolution comparison
- Training loss (red) and test loss (blue)
- Demonstrates functional convergence despite parameter divergence

### File Output

Results are saved as SVG files with descriptive filenames containing all experimental parameters:
```
"lr=0.01, σ=0.001, initial_permut=0.0, epochs=500, num_runs=10, seed=42.svg"
```